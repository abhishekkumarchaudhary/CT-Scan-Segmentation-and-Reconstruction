{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import scipy\n",
    "from skimage.transform import radon,iradon,rescale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading mask and CT_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data = scipy.io.loadmat(\"infmsk_hw1.mat\")\n",
    "mask = mask_data['infmsk']\n",
    "ct_data = scipy.io.loadmat(\"ctscan_hw1.mat\")\n",
    "ct = ct_data['ctscan']\n",
    "mask = np.transpose(mask, (2,0,1))\n",
    "ct = np.transpose(ct, (2,0,1))\n",
    "ct_scans=ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 \n",
    "Unet implementation from the link below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link: https://arxiv.org/abs/1505.04597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=3, depth=3, wf=6, padding=True,batch_norm=True, up_mode='upconv', residual=True):\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            if i == 0 and residual:\n",
    "                self.down_path.append(UNetConvBlock(prev_channels, 2 ** (wf + i),\n",
    "                                                    padding, batch_norm, residual, first=True))\n",
    "            else:\n",
    "                self.down_path.append(UNetConvBlock(prev_channels, 2 ** (wf + i),\n",
    "                                                    padding, batch_norm, residual))\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode,\n",
    "                                            padding, batch_norm, residual))\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        res = self.last(x)\n",
    "        return self.softmax(res)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm, residual=False, first=False):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        self.batch_norm = batch_norm\n",
    "        self.first = first\n",
    "        self.residual_input_conv = nn.Conv2d(self.in_size, self.out_size, kernel_size=1)\n",
    "        self.residual_batchnorm = nn.BatchNorm2d(self.out_size)\n",
    "\n",
    "        if residual:\n",
    "            padding = 1\n",
    "        block = []\n",
    "\n",
    "        if residual and not first:\n",
    "            block.append(nn.ReLU())\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm2d(in_size))\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "\n",
    "        if not residual:\n",
    "            block.append(nn.ReLU())\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm2d(out_size))\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if self.residual:\n",
    "            if self.in_size != self.out_size:\n",
    "                x = self.residual_input_conv(x)\n",
    "                x = self.residual_batchnorm(x)\n",
    "            out = out + x\n",
    "\n",
    "        return out\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm, residual=False):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.residual_input_conv = nn.Conv2d(self.in_size, self.out_size, kernel_size=1)\n",
    "        self.residual_batchnorm = nn.BatchNorm2d(self.out_size)\n",
    "\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n",
    "                                         stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    @staticmethod\n",
    "    def center_crop(layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out_orig = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out_orig)\n",
    "        if self.residual:\n",
    "            if self.in_size != self.out_size:\n",
    "                out_orig = self.residual_input_conv(out_orig)\n",
    "                out_orig = self.residual_batchnorm(out_orig)\n",
    "            out = out + out_orig\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader for test, eval etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ct = ct\n",
    "        self.mask = mask\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.ct[index]).unsqueeze(0), torch.from_numpy(self.mask[index]).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):                \n",
    "        return self.ct.shape[0]\n",
    "\n",
    "train_ds = myDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-train-val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "n = mask.shape[0]\n",
    "index=np.array(range(0,int(n)))\n",
    "index=np.random.permutation(n)\n",
    "a=index[0,int(0.7*n)]\n",
    "b=index[int(0.7*n),int(0.9*n)]\n",
    "c=index[int(0.9*n),int(1.0*n)]\n",
    "train_sampler = SubsetRandomSampler(a)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, sampler=train_sampler,shuffle=True)\n",
    "\n",
    "val_sampler = SubsetRandomSampler(b)\n",
    "val_loader = DataLoader(train_ds, batch_size=1, sampler=val_sampler,shuffle=True)\n",
    "\n",
    "test_sampler = SubsetRandomSampler(c)\n",
    "test_loader = DataLoader(train_ds, batch_size=1, sampler=test_sampler,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "unet = UNet(1, 3)\n",
    "# unet.cuda(1)\n",
    "optim = torch.optim.Adam(unet.parameters(), lr=1e-3)\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "epochs = 1\n",
    "for ep in range(epochs):\n",
    "    unet.train()\n",
    "    for x, y in tqdm(train_dl):\n",
    "#         x = x.cuda(1)\n",
    "#         y = y.cuda(1)\n",
    "        optim.zero_grad()\n",
    "        yt = unet(x.float())\n",
    "        loss = loss_fn2(yt, y.long().squeeze(1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        v_loss = 0\n",
    "        for x, y in tqdm(val_dl):\n",
    "#             x = x.cuda(1)\n",
    "#             y = y.cuda(1)\n",
    "            yt = unet(x.float())\n",
    "            loss = loss_fn2(yt, y.long().squeeze(1))\n",
    "            v_loss += loss.item()\n",
    "    print(v_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def give_metrics(y, y_pred):\n",
    "\n",
    "  N = y_pred.shape[0]\n",
    "  total_infection_sensitivity = 0\n",
    "  total_infection_specificity = 0\n",
    "  total_infection_accuracy = 0\n",
    "  total_infection_dice_score = 0\n",
    "  \n",
    "  total_normal_sensitivity = 0\n",
    "  total_normal_specificity = 0\n",
    "  total_normal_accuracy = 0\n",
    "  total_normal_dice_score = 0\n",
    "\n",
    "  count_infection_sensitivity = 0               \n",
    "\n",
    "  for i in range(y_pred.shape[0]):\n",
    "    infection_sensitivity = 0\n",
    "    #Getting confusion matrix\n",
    "    confusion_metric = (confusion_matrix(y[i].flatten(), y_pred[i].flatten(),labels=[0,1,2])).T\n",
    "\n",
    "    #Getting TP/FP/FN/TN\n",
    "    TP_infection = confusion_metric[1][1]\n",
    "    TP_normal = confusion_metric[2][2]\n",
    "\n",
    "    TN_infection = confusion_metric[0][0] + confusion_metric[2][0] + confusion_metric[0][2] + confusion_metric[2][2]\n",
    "    TN_normal = confusion_metric[0][0] + confusion_metric[0][1] + confusion_metric[1][0] + confusion_metric[1][1]\n",
    "    \n",
    "    FP_infection = confusion_metric[1][0] + confusion_metric[1][2]\n",
    "    FP_normal = confusion_metric[2][0] + confusion_metric[2][1] \n",
    "\n",
    "    FN_infection = confusion_metric[0][1] + confusion_metric[2][1]\n",
    "    FN_normal = confusion_metric[0][2] + confusion_metric[1][2]\n",
    "        \n",
    "    #calculating metrics    \n",
    "    infection_specificity = (TN_infection)/(TN_infection+FP_infection)\n",
    "    infection_accuracy = (TP_infection+TN_infection)/(TP_infection+TN_infection+FP_infection+FN_infection)\n",
    "    infection_dice_score = (2*TP_infection)/(2*TP_infection + FP_infection + FN_infection)\n",
    "\n",
    "    normal_sensitivity = (TP_normal)/(TP_normal+FN_normal)\n",
    "    normal_specificity = (TN_normal)/(TN_normal+FP_normal)\n",
    "    normal_accuracy = (TP_normal+TN_normal)/(TP_normal+TN_normal+FP_normal+FN_normal)\n",
    "    normal_dice_score = (2*TP_normal)/(2*TP_normal + FP_normal + FN_normal)\n",
    "    \n",
    "    if((TP_infection+FN_infection)!=0):\n",
    "      count_infection_sensitivity += 1\n",
    "      infection_sensitivity = (TP_infection)/(TP_infection+FN_infection)\n",
    "    \n",
    "    # normal_sensitivity = 0\n",
    "    # if((normal_TP+infectio_FN)!=0):\n",
    "    #   count_infection_sensitivity += 1\n",
    "    #   infection_sensitivity = (infection_TP)/(infection_TP+infection_FN)\n",
    "\n",
    "    total_infection_sensitivity += infection_sensitivity\n",
    "    total_normal_sensitivity += normal_sensitivity\n",
    "    total_infection_specificity += infection_specificity\n",
    "    total_normal_specificity += normal_specificity\n",
    "    total_infection_accuracy += infection_accuracy\n",
    "    total_normal_accuracy += normal_accuracy\n",
    "    total_infection_dice_score += infection_dice_score\n",
    "    total_normal_dice_score += normal_dice_score\n",
    "  #finding avg and printing \n",
    "  print(f\"Averaged dice score = {total_normal_dice_score/N}, sensitivity = {total_normal_sensitivity/N}, \\n specificity = {total_normal_specificity/N}, and accuracy = {total_normal_accuracy/N} for the normal\")\n",
    "  print(f\"\\nAveraged dice score = {total_infection_dice_score/N}, sensitivity = {total_infection_sensitivity/(count_infection_sensitivity+0.0001)}, \\n specificity = {total_infection_specificity/N}, and accuracy = {total_infection_accuracy/N} for the infection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "def eval(unet1):\n",
    "    unet1.eval()\n",
    "\n",
    "    y_all=[]\n",
    "    y_pred_all=[]\n",
    "    for x, y in val_dl:\n",
    "#         x = x.cuda(1)\n",
    "        with torch.no_grad():\n",
    "            y_pred = unet1(x.float())\n",
    "        y_all.append(y)\n",
    "        y_pred_all.append(yt)\n",
    "\n",
    "    y_all=np.array(y_all)\n",
    "    y_pred_all=np.array(y_pred_all)\n",
    "    y_all=y_all.squeeze()\n",
    "    y_pred_all =  yt_pred_all.argmax(dim=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    give_metrics(y_all, y_pred_all)\n",
    "    \n",
    "    #plotting \n",
    "    index_1=5\n",
    "    index_2=9\n",
    "    f, plot = plt.subplots(1,3)\n",
    "    plot[0].set_title(\"CT Scans\")\n",
    "    plot[0].imshow(ct_scans[index_1], cmap='gray')\n",
    "    plot[1].set_title(\"Expert infection mask\")\n",
    "    plot[1].imshow(mask[index_1], cmap='gray')\n",
    "    plot[2].set_title(\"Predicted Masks\")\n",
    "    plot[2].imshow(y_pred_all[index_1], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "eval(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "Reconstruction limited angle sinogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_reconstruction(unet1):\n",
    "\n",
    "    y_4x_all=[]\n",
    "    mask_4x_all=[]\n",
    "    y_8x_all=[]\n",
    "    mask_8x_all=[]\n",
    "    \n",
    "    for scan, mask_ in tqdm(val_dl):\n",
    "        scan = scan.squeeze(0).squeeze(0).cpu().numpy()\n",
    "        organ_area = (mask_.squeeze().cpu().numpy()>0).astype(int)\n",
    "        sinogram = radon(scan, circle=False, preserve_range=True)\n",
    "        \n",
    "        #Reconstruct the CT scans from limited angle sinograms 4x\n",
    "        sinogram_4x = np.array([sinogram[:, i] for i in range(0, 180, 4)])\n",
    "        reconstruction4x = iradon(sinogram_4x.T, circle=False, preserve_range=True)\n",
    "        with torch.no_grad():\n",
    "            y_4x = unet1(torch.from_numpy(reconstruction4x*organ_area).unsqueeze(0).unsqueeze(0).cuda(1).float())\n",
    "        y_4x_all.append(y_4x.argmax(dim=1).squeeze(0).detach().cpu().numpy())\n",
    "        mask_4x_all.append(mask_)\n",
    "        \n",
    "        #Reconstruct the CT scans from limited angle sinograms 4x\n",
    "        sinogram_8x = np.array([sinogram[:, i] for i in range(0, 180, 8)])\n",
    "        reconstruction8x = iradon(sinogram_8x.T, circle=False, preserve_range=True)\n",
    "        with torch.no_grad():\n",
    "            y_8x = unet1(torch.from_numpy(reconstruction8x*organ_area).unsqueeze(0).unsqueeze(0).cuda(1).float())\n",
    "        y_8x_all.append(y_8x.argmax(dim=1).squeeze(0).detach().cpu().numpy())\n",
    "        mask_8x_all.append(mask_)\n",
    "\n",
    "    psnr_4x = peak_signal_noise_ratio(scan, reconstruction4x)\n",
    "    ssim_4x = structural_similarity(scan, reconstruction4x)\n",
    "    give_metrics(np.array(mask_4x), np.array(y_4x_all))\n",
    "    print(psnr_4x,ssim_4x)\n",
    "    \n",
    "    psnr_8x = peak_signal_noise_ratio(scan, reconstruction4x)\n",
    "    ssim_8x = structural_similarity(scan, reconstruction4x)\n",
    "    give_metrics(np.array(mask_8x), np.array(y_8x_all))\n",
    "    print(psnr_8x,ssim_8x)\n",
    "    \n",
    "    #PLOTTING THE RECONSTRUCTED SINOGRAMS\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "    i = 5\n",
    "    f, axarr = plt.subplots(1,3)\n",
    "    axarr[0].set_title(\"4x Reconstruction\")\n",
    "    axarr[0].imshow(reconstructed_ct_scans_4x[i], cmap='gray')\n",
    "    axarr[1].set_title(\"8x Reconstruction\")\n",
    "    axarr[1].imshow(reconstructed_ct_scans_8x[i], cmap='gray')\n",
    "    axarr[2].set_title(\"CT Scans\")\n",
    "    axarr[2].imshow(ct_scans[i], cmap='gray')\n",
    "    f.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "eval_reconstruction(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qn3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "x = random.randint(0, 3554)\n",
    "scan = torch.from_numpy(ct[x]).cuda(1)\n",
    "mask_ = torch.from_numpy(mask[x]).cuda(1)\n",
    "\n",
    "for eta in [-0.01, -0.001, 0.001, 0.01]:\n",
    "    new_model = UNet_model(1, 3)\n",
    "    new_model.load_state_dict(torch.load('unet_modelxapoo9.756179238078403.pt'))\n",
    "#     new_model.cuda(1)\n",
    "    with torch.no_grad():\n",
    "        for name, param in new_model.named_parameters():\n",
    "            modified_param = (1+eta)*param\n",
    "            param.copy_(modified_param)\n",
    "\n",
    "    eval(new_model)\n",
    "\n",
    "    # display sample\n",
    "    with torch.no_grad():\n",
    "        yt = new_model(scan.unsqueeze(0).unsqueeze(0).float())\n",
    "    yt = yt.argmax(dim=1).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    #plotting \n",
    "    index_1=20\n",
    "    f, plot = plt.subplots(1,3)\n",
    "    plot[0].set_title(\"CT Scans\")\n",
    "    plot[0].imshow(ct_scans[index_1], cmap='gray')\n",
    "    plot[1].set_title(\"Expert infection mask\")\n",
    "    plot[1].imshow(mask[index_1], cmap='gray')\n",
    "    plot[2].set_title(\"Predicted Masks\")\n",
    "    plot[2].imshow(y_pred_all[index_1], cmap='gray')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d8d907076e08b2d0f8705db29e0a3a5a50916422e2db334c7cc53ba93d6601b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
